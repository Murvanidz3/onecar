import os
import uvicorn
from curl_cffi import requests as cffi_requests
from bs4 import BeautifulSoup
from fastapi import FastAPI
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pydantic import BaseModel
import re

app = FastAPI()

app.mount("/static", StaticFiles(directory="static"), name="static")

class VinRequest(BaseModel):
    vin: str

def scrape_bidcars(vin):
    try:
        # 1. áƒ«áƒ”áƒ‘áƒœáƒ VIN áƒ™áƒáƒ“áƒ˜áƒ—
        search_url = f"https://bid.cars/en/search/results?search-term={vin}"
        print(f"Searching: {search_url}")
        
        response = cffi_requests.get(search_url, impersonate="chrome")
        if response.status_code != 200:
            return {"error": "áƒ•áƒ”áƒ  áƒ“áƒáƒ•áƒ£áƒ™áƒáƒ•áƒ¨áƒ˜áƒ áƒ“áƒ˜ Bid.cars-áƒ¡"}

        soup = BeautifulSoup(response.content, 'html.parser')
        
        # áƒ•áƒ”áƒ«áƒ”áƒ‘áƒ— áƒ¨áƒ”áƒ“áƒ”áƒ’áƒ¡ (áƒ›áƒáƒœáƒ¥áƒáƒœáƒ˜áƒ¡ áƒšáƒ˜áƒœáƒ™áƒ¡)
        # áƒ©áƒ•áƒ”áƒ£áƒšáƒ”áƒ‘áƒ áƒ˜áƒ• áƒ”áƒ¡ áƒáƒ áƒ˜áƒ¡ 'view-auction' áƒ¦áƒ˜áƒšáƒáƒ™áƒ˜ áƒáƒœ áƒšáƒ˜áƒœáƒ™áƒ˜ áƒ¡áƒ˜áƒáƒ¨áƒ˜
        car_link = None
        results = soup.find_all('a', href=True)
        for link in results:
            if "/lot/" in link['href']:
                car_link = link['href']
                break
        
        if not car_link:
             # áƒ–áƒáƒ’áƒ¯áƒ”áƒ  áƒáƒ˜áƒ áƒ“áƒáƒáƒ˜áƒ  áƒ›áƒáƒœáƒ¥áƒáƒœáƒ˜áƒ¡ áƒ’áƒ•áƒ”áƒ áƒ“áƒ–áƒ” áƒ’áƒáƒ“áƒáƒ°áƒ§áƒáƒ•áƒ¡
             if "/lot/" in response.url:
                 car_link = response.url
             else:
                 return {"error": "áƒ›áƒáƒœáƒ¥áƒáƒœáƒ áƒ•áƒ”áƒ  áƒ›áƒáƒ˜áƒ«áƒ”áƒ‘áƒœáƒ áƒáƒ áƒ¥áƒ˜áƒ•áƒ¨áƒ˜ ğŸ¤·â€â™‚ï¸"}

        # áƒ¡áƒ áƒ£áƒšáƒ˜ áƒšáƒ˜áƒœáƒ™áƒ˜áƒ¡ áƒáƒ¬áƒ§áƒáƒ‘áƒ
        if not car_link.startswith("http"):
            full_link = f"https://bid.cars{car_link}"
        else:
            full_link = car_link

        print(f"Found Page: {full_link}")

        # 2. áƒ¨áƒ”áƒ•áƒ“áƒ˜áƒ•áƒáƒ áƒ— áƒ›áƒáƒœáƒ¥áƒáƒœáƒ˜áƒ¡ áƒ’áƒ•áƒ”áƒ áƒ“áƒ–áƒ”
        page_response = cffi_requests.get(full_link, impersonate="chrome")
        page_soup = BeautifulSoup(page_response.content, 'html.parser')

        # 3. áƒ›áƒáƒœáƒáƒªáƒ”áƒ›áƒ”áƒ‘áƒ˜áƒ¡ áƒáƒ›áƒáƒ¦áƒ”áƒ‘áƒ
        data = {
            "title": "áƒ£áƒªáƒœáƒáƒ‘áƒ˜",
            "images": [],
            "info": {}
        }

        # áƒ¡áƒáƒ—áƒáƒ£áƒ áƒ˜
        h1 = page_soup.find('h1')
        if h1: data['title'] = h1.get_text(strip=True)

        # áƒ¤áƒáƒ¢áƒáƒ”áƒ‘áƒ˜ (Gallery)
        # bid.cars-áƒ–áƒ” áƒ¤áƒáƒ¢áƒáƒ”áƒ‘áƒ˜ áƒ®áƒ¨áƒ˜áƒ áƒáƒ“ áƒáƒ áƒ˜áƒ¡ "galleria" áƒáƒœ "owl-carousel" áƒ™áƒšáƒáƒ¡áƒ”áƒ‘áƒ¨áƒ˜
        images = []
        img_tags = page_soup.find_all('img')
        for img in img_tags:
            src = img.get('src') or img.get('data-src')
            if src and "media.bid.cars" in src and "small" not in src:
                # áƒ•áƒªáƒ“áƒ˜áƒšáƒáƒ‘áƒ— áƒ“áƒ˜áƒ“áƒ˜ áƒ–áƒáƒ›áƒ˜áƒ¡ áƒ¤áƒáƒ¢áƒáƒ”áƒ‘áƒ˜ áƒáƒ•áƒ˜áƒ¦áƒáƒ—
                full_size = src.replace("thumbnails/", "").replace("small/", "")
                if full_size not in images:
                    images.append(full_size)
        
        # áƒ•áƒ˜áƒ¦áƒ”áƒ‘áƒ— áƒ›áƒ®áƒáƒšáƒáƒ“ áƒáƒ˜áƒ áƒ•áƒ”áƒš 5-6 áƒ¤áƒáƒ¢áƒáƒ¡, áƒ áƒáƒ› áƒáƒ  áƒ’áƒáƒ“áƒáƒ˜áƒ¢áƒ•áƒ˜áƒ áƒ—áƒáƒ¡
        data['images'] = images[:6]

        # áƒ¢áƒ”áƒ¥áƒœáƒ˜áƒ™áƒ£áƒ áƒ˜ áƒ˜áƒœáƒ¤áƒ (áƒªáƒ®áƒ áƒ˜áƒšáƒ˜áƒ“áƒáƒœ)
        # áƒ•áƒ”áƒ«áƒ”áƒ‘áƒ— áƒ•áƒ”áƒšáƒ”áƒ‘áƒ¡: Primary Damage, Odometer, etc.
        info_block = page_soup.get_text()
        
        # áƒ›áƒáƒ áƒ¢áƒ˜áƒ•áƒ˜ Regex áƒ«áƒ”áƒ‘áƒœáƒ áƒ¢áƒ”áƒ¥áƒ¡áƒ¢áƒ¨áƒ˜
        odometer = re.search(r'Odometer[:\s]+([\d,]+)', info_block)
        damage = re.search(r'Primary Damage[:\s]+([A-Za-z\s]+)', info_block)
        engine = re.search(r'Engine[:\s]+([0-9\.]+L)', info_block)

        if odometer: data['info']['odometer'] = odometer.group(1)
        if damage: data['info']['damage'] = damage.group(1).strip()
        if engine: data['info']['engine'] = engine.group(1)

        return data

    except Exception as e:
        print(f"Error scraping: {e}")
        return {"error": str(e)}

@app.get("/")
def read_root():
    return FileResponse('static/index.html')

# áƒáƒ®áƒáƒšáƒ˜ áƒ”áƒœáƒ“áƒáƒáƒ˜áƒœáƒ¢áƒ˜ VIN-áƒ˜áƒ¡ áƒ¨áƒ”áƒ¡áƒáƒ›áƒáƒ¬áƒ›áƒ”áƒ‘áƒšáƒáƒ“
@app.post("/check_vin")
def check_vin_handler(req: VinRequest):
    result = scrape_bidcars(req.vin)
    return result

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)